{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "78754258afacad16fffc70116fd218f31174e48a87ca49a016c8c60e859fb741"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간헐적으로 발생하는 에러 방지\n",
    "# IOError: [Errno socket error] [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n\n\n\n\n\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\nof steam!\n\n\n\n\n\n\n\nJim,\n\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\ndenial about the faith you need to get by.  Oh well, just pretend that it will\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\nalt.atheist.hard, you won't be bummin' so much?\n\n\n\n\n\n\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n--\nBake Timmons, III\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n11314\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "print(documents[1])\n",
    "print(dataset.target_names)\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/bagjeongtae/Desktop/SVD/venv/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "# 특수문자 제거, 길이가 3이하인 단어 제고, 대문자 -> 소문자 변환\n",
    "\n",
    "new_df = pd.DataFrame({'document': documents})\n",
    "new_df['clean_doc'] = new_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "new_df['clean_doc'] = new_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "new_df['clean_doc'] = new_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\nI disagree with is your statement that the U.S. Media is out to\nruin Israels reputation. That is rediculous. The U.S. media is\nthe most pro-israeli media in the world. Having lived in Europe\nI realize that incidences such as the one described in the\nletter have occured. The U.S. media as a whole seem to try to\nignore them. The U.S. is subsidizing Israels existance and the\nEuropeans are not (at least not to the same degree). So I think\nthat might be a reason they report more clearly on the\natrocities.\n\tWhat is a shame is that in Austria, daily reports of\nthe inhuman acts commited by Israeli soldiers and the blessing\nreceived from the Government makes some of the Holocaust guilt\ngo away. After all, look how the Jews are treating other races\nwhen they got power. It is unfortunate.\n\n==================================\nwell sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate\n"
     ]
    }
   ],
   "source": [
    "print(new_df['document'][0])\n",
    "print('==================================')\n",
    "print(new_df['clean_doc'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = new_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['well',\n",
       " 'sure',\n",
       " 'story',\n",
       " 'seem',\n",
       " 'biased',\n",
       " 'disagree',\n",
       " 'statement',\n",
       " 'media',\n",
       " 'ruin',\n",
       " 'israels',\n",
       " 'reputation',\n",
       " 'rediculous',\n",
       " 'media',\n",
       " 'israeli',\n",
       " 'media',\n",
       " 'world',\n",
       " 'lived',\n",
       " 'europe',\n",
       " 'realize',\n",
       " 'incidences',\n",
       " 'described',\n",
       " 'letter',\n",
       " 'occured',\n",
       " 'media',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'ignore',\n",
       " 'subsidizing',\n",
       " 'israels',\n",
       " 'existance',\n",
       " 'europeans',\n",
       " 'least',\n",
       " 'degree',\n",
       " 'think',\n",
       " 'might',\n",
       " 'reason',\n",
       " 'report',\n",
       " 'clearly',\n",
       " 'atrocities',\n",
       " 'shame',\n",
       " 'austria',\n",
       " 'daily',\n",
       " 'reports',\n",
       " 'inhuman',\n",
       " 'acts',\n",
       " 'commited',\n",
       " 'israeli',\n",
       " 'soldiers',\n",
       " 'blessing',\n",
       " 'received',\n",
       " 'government',\n",
       " 'makes',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'away',\n",
       " 'look',\n",
       " 'jews',\n",
       " 'treating',\n",
       " 'races',\n",
       " 'power',\n",
       " 'unfortunate']"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tokenized_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons\n"
     ]
    }
   ],
   "source": [
    "# 역토큰화 \n",
    "detokenized_doc = []\n",
    "\n",
    "for idx, tokenized in enumerate(tokenized_doc):\n",
    "   detokenized_doc.append(' '.join(tokenized))\n",
    "\n",
    "print(detokenized_doc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 663)\t0.12482699217899616\n  (0, 444)\t0.15920786810198112\n  (0, 497)\t0.11606278893174055\n  (0, 73)\t0.13042573485326867\n  (0, 516)\t0.13226489554872167\n  (0, 366)\t0.12704052323102857\n  (0, 716)\t0.15952691353774057\n  (0, 814)\t0.1846235059899824\n  (0, 732)\t0.16381332141612362\n  (0, 153)\t0.1582696239800608\n  (0, 730)\t0.16151197900355305\n  (0, 712)\t0.128507139034102\n  (0, 893)\t0.0880418781548271\n  (0, 477)\t0.16834732334900732\n  (0, 228)\t0.17099938915691926\n  (0, 710)\t0.16609758611560899\n  (0, 388)\t0.12393127093119301\n  (0, 985)\t0.12177290184053344\n  (0, 438)\t0.34482777905476636\n  (0, 532)\t0.6620433868499663\n  (0, 841)\t0.15289850944254976\n  (0, 849)\t0.1573584484023006\n  (0, 867)\t0.11171887866892631\n  (1, 337)\t0.2149573671240573\n  (1, 581)\t0.20790058182513171\n  :\t:\n  (11313, 184)\t0.15687735058786417\n  (11313, 910)\t0.16737324865300288\n  (11313, 98)\t0.14269547275781186\n  (11313, 944)\t0.14004400080758297\n  (11313, 517)\t0.14260924213040407\n  (11313, 694)\t0.13737279229913762\n  (11313, 931)\t0.10500616461965587\n  (11313, 997)\t0.22769671291780222\n  (11313, 728)\t0.1355912879455921\n  (11313, 714)\t0.16177960777248834\n  (11313, 821)\t0.15810451317180033\n  (11313, 589)\t0.1528561180552639\n  (11313, 134)\t0.14548932449907176\n  (11313, 472)\t0.16505305772348522\n  (11313, 365)\t0.28837622091772286\n  (11313, 746)\t0.10675084794292293\n  (11313, 996)\t0.11881608206978105\n  (11313, 114)\t0.1410130381587366\n  (11313, 128)\t0.13902593736075236\n  (11313, 951)\t0.10471094881917142\n  (11313, 458)\t0.08415091747543026\n  (11313, 483)\t0.08061362550823113\n  (11313, 448)\t0.1635795686275028\n  (11313, 9)\t0.12318700766629886\n  (11313, 301)\t0.1551830624693101\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 적용\n",
    "# TF-IDF는 1000개 단어 제한하여 만듬\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, max_df = 0.5, smooth_idf=True)\n",
    "X = vectorizer.fit_transform(new_df['clean_doc'])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.01  0.05  0.02 ...  0.07  0.01  0.02]\n [-0.01  0.02 -0.02 ... -0.06 -0.01 -0.02]\n [ 0.   -0.   -0.02 ...  0.06  0.02  0.02]\n ...\n [ 0.01 -0.01 -0.01 ... -0.03 -0.01 -0.  ]\n [ 0.    0.01 -0.01 ...  0.01 -0.02 -0.  ]\n [-0.    0.01  0.01 ... -0.05 -0.01 -0.  ]]\n[0.01 0.05 0.02 0.03 0.02 0.01 0.02 0.01 0.01 0.06 0.01 0.01 0.01 0.03\n 0.01 0.04 0.02 0.01 0.04 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.01 0.01\n 0.01 0.01 0.04 0.01 0.02 0.04 0.02 0.01 0.01 0.03 0.02 0.02 0.01 0.02\n 0.03 0.01 0.01 0.02 0.01 0.01 0.03 0.01 0.02 0.02 0.01 0.01 0.02 0.02\n 0.01 0.01 0.04 0.01 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.05 0.02 0.02 0.04 0.01 0.02 0.02 0.04 0.01 0.02 0.01 0.01 0.01 0.01\n 0.01 0.08 0.06 0.07 0.04 0.04 0.01 0.01 0.02 0.01 0.02 0.01 0.03 0.03\n 0.04 0.02 0.01 0.03 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.01 0.05\n 0.01 0.01 0.03 0.01 0.06 0.02 0.03 0.02 0.03 0.06 0.02 0.03 0.02 0.03\n 0.03 0.02 0.04 0.02 0.01 0.   0.01 0.02 0.03 0.01 0.02 0.03 0.04 0.02\n 0.02 0.03 0.03 0.02 0.03 0.03 0.01 0.02 0.01 0.03 0.02 0.02 0.02 0.02\n 0.02 0.03 0.01 0.02 0.03 0.03 0.01 0.01 0.06 0.03 0.02 0.01 0.01 0.02\n 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.02 0.02 0.05 0.01 0.02 0.01\n 0.01 0.03 0.02 0.02 0.02 0.01 0.01 0.01 0.   0.04 0.02 0.01 0.03 0.03\n 0.03 0.01 0.02 0.01 0.01 0.03 0.03 0.06 0.02 0.01 0.02 0.01 0.02 0.01\n 0.01 0.04 0.02 0.05 0.02 0.02 0.03 0.03 0.02 0.03 0.02 0.02 0.01 0.01\n 0.02 0.01 0.03 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.01 0.01 0.02 0.01\n 0.07 0.01 0.03 0.05 0.02 0.01 0.01 0.02 0.01 0.02 0.01 0.04 0.01 0.03\n 0.01 0.01 0.01 0.15 0.07 0.05 0.01 0.01 0.02 0.01 0.07 0.03 0.03 0.02\n 0.02 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.03 0.01 0.02 0.01 0.01 0.01\n 0.05 0.03 0.01 0.02 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.03 0.01 0.01\n 0.03 0.03 0.04 0.02 0.02 0.01 0.01 0.02 0.01 0.01 0.03 0.02 0.01 0.02\n 0.02 0.05 0.02 0.02 0.01 0.01 0.02 0.03 0.02 0.01 0.01 0.02 0.03 0.01\n 0.02 0.06 0.04 0.01 0.02 0.03 0.01 0.02 0.02 0.02 0.04 0.01 0.01 0.02\n 0.02 0.02 0.02 0.02 0.04 0.01 0.03 0.01 0.01 0.01 0.01 0.02 0.06 0.04\n 0.02 0.03 0.02 0.01 0.01 0.03 0.05 0.04 0.02 0.01 0.01 0.01 0.03 0.08\n 0.02 0.14 0.06 0.03 0.06 0.02 0.01 0.02 0.05 0.02 0.04 0.02 0.02 0.02\n 0.03 0.01 0.01 0.02 0.02 0.02 0.02 0.05 0.03 0.03 0.05 0.02 0.01 0.03\n 0.05 0.01 0.03 0.02 0.07 0.05 0.02 0.02 0.02 0.02 0.01 0.03 0.04 0.01\n 0.02 0.03 0.05 0.02 0.02 0.01 0.02 0.03 0.02 0.02 0.02 0.02 0.01 0.01\n 0.01 0.04 0.06 0.02 0.02 0.02 0.03 0.01 0.01 0.01 0.04 0.03 0.01 0.02\n 0.01 0.02 0.02 0.03 0.02 0.03 0.01 0.01 0.05 0.02 0.03 0.01 0.03 0.\n 0.21 0.01 0.02 0.02 0.02 0.02 0.01 0.04 0.01 0.02 0.19 0.02 0.03 0.02\n 0.01 0.02 0.01 0.03 0.01 0.03 0.01 0.01 0.02 0.01 0.02 0.02 0.02 0.04\n 0.02 0.01 0.03 0.02 0.01 0.05 0.03 0.2  0.03 0.01 0.05 0.02 0.05 0.07\n 0.03 0.01 0.02 0.04 0.02 0.07 0.02 0.06 0.02 0.06 0.03 0.01 0.02 0.02\n 0.03 0.01 0.01 0.03 0.02 0.01 0.07 0.01 0.02 0.02 0.01 0.1  0.04 0.03\n 0.02 0.02 0.01 0.02 0.02 0.01 0.01 0.01 0.03 0.01 0.05 0.05 0.01 0.04\n 0.02 0.02 0.01 0.01 0.04 0.02 0.02 0.03 0.01 0.01 0.01 0.02 0.02 0.02\n 0.02 0.02 0.03 0.02 0.01 0.01 0.03 0.03 0.01 0.02 0.01 0.04 0.03 0.01\n 0.02 0.01 0.02 0.01 0.02 0.02 0.01 0.01 0.01 0.01 0.02 0.02 0.01 0.02\n 0.02 0.02 0.1  0.02 0.02 0.02 0.03 0.02 0.04 0.02 0.02 0.01 0.03 0.\n 0.06 0.02 0.01 0.01 0.02 0.02 0.03 0.01 0.02 0.01 0.03 0.03 0.01 0.02\n 0.02 0.01 0.01 0.01 0.04 0.01 0.04 0.02 0.02 0.02 0.01 0.01 0.01 0.02\n 0.02 0.01 0.02 0.01 0.01 0.01 0.02 0.01 0.02 0.02 0.18 0.02 0.02 0.02\n 0.04 0.02 0.01 0.04 0.01 0.02 0.01 0.01 0.04 0.02 0.01 0.04 0.01 0.02\n 0.03 0.02 0.02 0.07 0.02 0.02 0.01 0.02 0.01 0.01 0.02 0.02 0.05 0.02\n 0.06 0.02 0.03 0.02 0.01 0.06 0.01 0.02 0.02 0.02 0.04 0.01 0.01 0.04\n 0.01 0.02 0.01 0.02 0.06 0.09 0.05 0.02 0.01 0.01 0.02 0.01 0.06 0.01\n 0.02 0.02 0.01 0.01 0.01 0.02 0.01 0.01 0.04 0.01 0.02 0.02 0.07 0.03\n 0.05 0.01 0.02 0.01 0.02 0.01 0.06 0.03 0.05 0.01 0.02 0.09 0.04 0.02\n 0.02 0.01 0.01 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.02 0.01 0.03 0.02\n 0.04 0.03 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.02 0.01 0.01 0.01 0.02\n 0.02 0.02 0.02 0.02 0.1  0.03 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.04\n 0.02 0.01 0.01 0.01 0.02 0.08 0.03 0.01 0.02 0.03 0.04 0.02 0.02 0.01\n 0.03 0.03 0.01 0.03 0.05 0.01 0.01 0.01 0.02 0.05 0.02 0.03 0.04 0.03\n 0.02 0.02 0.02 0.02 0.03 0.02 0.01 0.01 0.01 0.02 0.02 0.02 0.02 0.02\n 0.01 0.03 0.03 0.03 0.02 0.02 0.01 0.02 0.02 0.01 0.04 0.01 0.01 0.02\n 0.05 0.01 0.01 0.02 0.02 0.03 0.03 0.03 0.03 0.03 0.03 0.02 0.01 0.01\n 0.05 0.01 0.02 0.02 0.01 0.04 0.01 0.02 0.03 0.01 0.04 0.03 0.01 0.05\n 0.01 0.02 0.02 0.01 0.02 0.01 0.   0.02 0.03 0.02 0.   0.01 0.02 0.01\n 0.02 0.04 0.02 0.03 0.02 0.02 0.01 0.02 0.04 0.01 0.01 0.02 0.02 0.08\n 0.01 0.02 0.02 0.03 0.01 0.02 0.02 0.02 0.03 0.03 0.02 0.05 0.02 0.01\n 0.02 0.06 0.02 0.01 0.02 0.02 0.02 0.11 0.02 0.07 0.08 0.17 0.03 0.05\n 0.14 0.04 0.01 0.03 0.03 0.03 0.01 0.01 0.02 0.01 0.02 0.01 0.01 0.03\n 0.02 0.06 0.02 0.03 0.05 0.01 0.01 0.01 0.03 0.01 0.03 0.01 0.03 0.01\n 0.02 0.01 0.01 0.01 0.03 0.02 0.03 0.09 0.02 0.01 0.02 0.02 0.02 0.07\n 0.03 0.02 0.01 0.01 0.02 0.01 0.04 0.01 0.04 0.02 0.02 0.01 0.02 0.1\n 0.03 0.02 0.01 0.03 0.02 0.02 0.01 0.01 0.02 0.03 0.02 0.03 0.01 0.01\n 0.02 0.01 0.01 0.01 0.02 0.04 0.08 0.01 0.02 0.01 0.02 0.02 0.02 0.03\n 0.02 0.09 0.02 0.03 0.04 0.06 0.02 0.03 0.03 0.03 0.01 0.02 0.05 0.01\n 0.01 0.02 0.08 0.07 0.01 0.02]\n20\n1000\n"
     ]
    }
   ],
   "source": [
    "# 토픽 모델링\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=30, algorithm='randomized', n_iter=10000, random_state=122)\n",
    "svd_model.fit(X)\n",
    "\n",
    "print(svd_model.components_.round(2))\n",
    "print(svd_model.components_[0].round(2))\n",
    "print(len(svd_model.components_))\n",
    "print(len(svd_model.components_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actual',\n",
       " 'actually']"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# tf-idf로 벡터화 시킨 단어 리스트\n",
    "terms = vectorizer.get_feature_names()\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic 1: [('just', 0.20887), ('like', 0.20469), ('know', 0.19349), ('people', 0.18318), ('think', 0.1697)]\nTopic 2: [('thanks', 0.32763), ('windows', 0.28786), ('card', 0.18019), ('drive', 0.16864), ('mail', 0.15261)]\nTopic 3: [('game', 0.34011), ('team', 0.30311), ('year', 0.26894), ('games', 0.23784), ('drive', 0.17472)]\nTopic 4: [('drive', 0.46159), ('scsi', 0.17188), ('disk', 0.14451), ('hard', 0.13805), ('problem', 0.12763)]\nTopic 5: [('drive', 0.39993), ('know', 0.28768), ('thanks', 0.24917), ('does', 0.24678), ('just', 0.17387)]\nTopic 6: [('just', 0.55559), ('like', 0.23559), ('windows', 0.23078), ('know', 0.15795), ('does', 0.11156)]\nTopic 7: [('just', 0.43264), ('like', 0.22858), ('mail', 0.15052), ('bike', 0.11698), ('thanks', 0.10025)]\nTopic 8: [('does', 0.39692), ('know', 0.25192), ('chip', 0.22492), ('like', 0.17824), ('card', 0.15695)]\nTopic 9: [('like', 0.42065), ('card', 0.32249), ('sale', 0.20267), ('video', 0.1571), ('offer', 0.14119)]\nTopic 10: [('like', 0.61166), ('drive', 0.23998), ('file', 0.13257), ('files', 0.09233), ('sounds', 0.08533)]\nTopic 11: [('people', 0.44189), ('like', 0.25647), ('thanks', 0.19072), ('card', 0.18615), ('government', 0.18341)]\nTopic 12: [('think', 0.66867), ('good', 0.25984), ('thanks', 0.11249), ('need', 0.10479), ('chip', 0.09178)]\nTopic 13: [('think', 0.36273), ('does', 0.22264), ('just', 0.21194), ('mail', 0.12875), ('like', 0.12095)]\nTopic 14: [('know', 0.36546), ('good', 0.30975), ('people', 0.27825), ('windows', 0.2729), ('file', 0.20088)]\nTopic 15: [('space', 0.4519), ('know', 0.31141), ('think', 0.18571), ('nasa', 0.1711), ('card', 0.11724)]\nTopic 16: [('does', 0.42324), ('israel', 0.31948), ('think', 0.26996), ('right', 0.1863), ('israeli', 0.17007)]\nTopic 17: [('good', 0.41859), ('space', 0.27501), ('card', 0.1855), ('does', 0.16985), ('thanks', 0.16633)]\nTopic 18: [('people', 0.5173), ('does', 0.25965), ('window', 0.21657), ('problem', 0.19057), ('space', 0.12958)]\nTopic 19: [('right', 0.36578), ('bike', 0.31154), ('time', 0.19438), ('windows', 0.18916), ('space', 0.1777)]\nTopic 20: [('file', 0.53334), ('problem', 0.20198), ('files', 0.19583), ('need', 0.18333), ('time', 0.15802)]\n"
     ]
    }
   ],
   "source": [
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(svd_model.components_, terms) # VT를 기반으로 각 토픽당 상위 단어 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenzied_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}